import os
from typing import List, Optional
from tavily import AsyncTavilyClient
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from graph_type import GraphState
from pathlib import Path
from langchain_core.messages import SystemMessage, HumanMessage
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_groq import ChatGroq
load_dotenv()
# google_api_key=os.getenv("GOOGLE_API_KEY", "")
from llm import get_llm, stream_with_token_tracking
_tavily: Optional[AsyncTavilyClient] = None
from prompt_cache import normalize_prefix

if os.getenv("TAVILY_API_KEY"):
    try:
        _tavily = AsyncTavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
        print(f"[WebSearch] Tavily client initialized successfully")
    except Exception as e:
        print(f"[WebSearch] Failed to init Tavily: {e}")
        _tavily = None
else:
    print(f"[WebSearch] TAVILY_API_KEY not found in environment")

# At top of websearch.py
PROMPT_PATH = os.path.join(os.path.dirname(__file__), "websearch.md")
BASIC_PROMPT_PATH = os.path.join(os.path.dirname(__file__), "websearch_basic.md")
async def send_status_update(state: GraphState, message: str, progress: int = None):
    """Send status update if callback is available"""
    if hasattr(state, '_status_callback') and state._status_callback:
        await state._status_callback({
            "type": "status",
            "data": {
                "status": "processing",
                "message": message,
                "current_node": "WebSearch",
                "progress": progress
            }
        })
def load_prompt(path: str, fallback: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return fallback

WEBSEARCH_PROMPT = load_prompt(
    PROMPT_PATH,
    "You are a helpful assistant. Format results clearly with headings, bullets, numbered lists. Cite as [Source X]."
)

WEBSEARCH_BASIC_PROMPT = load_prompt(
    BASIC_PROMPT_PATH,
    "Provide a concise answer (3-10 sentences) based only on the user query and the search results. Cite as [Source X]."
)
# === Prompt caching setup for WebSearch ===
CORE_PREFIX_PATH = os.path.join(os.path.dirname(__file__), "..", "prompts", "core_prefix.md")
WEBSEARCH_PATH = os.path.join(os.path.dirname(__file__), "websearch.md")
WEBSEARCH_BASIC_PATH = os.path.join(os.path.dirname(__file__), "websearch_basic.md")

CORE_PREFIX = ""
WEB_RULES = ""
WEB_RULES_BASIC = ""

try:
    with open(CORE_PREFIX_PATH, "r", encoding="utf-8") as f:
        CORE_PREFIX = f.read()
except FileNotFoundError:
    CORE_PREFIX = "You are a reliable research assistant with live search capability."

try:
    with open(WEBSEARCH_PATH, "r", encoding="utf-8") as f:
        WEB_RULES = f.read()
except FileNotFoundError:
    WEB_RULES = "Format answers clearly with structure and source list."

try:
    with open(WEBSEARCH_BASIC_PATH, "r", encoding="utf-8") as f:
        WEB_RULES_BASIC = f.read()
except FileNotFoundError:
    WEB_RULES_BASIC = "Provide concise structured answers based on web search."

# Create normalized, cacheable static prefixes
STATIC_SYS_WEBSEARCH = normalize_prefix([CORE_PREFIX, WEB_RULES])
STATIC_SYS_WEBSEARCH_BASIC = normalize_prefix([CORE_PREFIX, WEB_RULES_BASIC])

async def web_search(query: str, max_results: int = 5, search_depth: str="basic") -> List[Document]:
    """Perform Tavily web search and return results as LangChain Documents."""
    print(f"[WebSearch] Starting web search for: {query}")
    
    
    if not _tavily:
        print(f"[WebSearch] No Tavily client available")
        return []

    try:
        print(f"[WebSearch] Calling Tavily search API...")
        results = await _tavily.search(query=query, max_results=max_results, search_depth=search_depth)
        # print(f"[WebSearch] Raw Tavily response: {results}")
        
        docs = []
        for r in results.get("results", []):
            docs.append(
                Document(
                    page_content=r.get("content", ""),
                    metadata={"title": r.get("title", ""), "url": r.get("url", "")}
                )
            )
        print(f"[WebSearch] Created {len(docs)} documents from search results")
        return docs
    except Exception as e:
        print(f"[WebSearch] Error performing search: {e}")
        import traceback
        traceback.print_exc()
        return []


def is_web_search_available() -> bool:
    """Check if Tavily client is available."""
    return _tavily is not None



async def run_web_search(state: GraphState) -> GraphState:
    """
    Graph node for Tavily web search:
    - Fetch results
    - Format them into sources text
    - Send to LLM with websearch.md prompt
    - Store structured answer in state["response"]
    """
    query = state.get("resolved_query") or state.get("user_query", "")
    is_web_search=state.get("web_search", False)
    state["active_docs"]=None
    system_prompt = WEBSEARCH_PROMPT if is_web_search else WEBSEARCH_BASIC_PROMPT

    llm_model = state.get("llm_model", "gpt-4o")
    print(f"[WebSearch] Starting run_web_search for query: {query}")
    
    if not query:
        state["response"] = "No query provided for web search."
        print(f"[WebSearch] No query provided")
        return state
    await send_status_update(state, "üåê Gathering information from websites...", 20)
    print(f"[WebSearch] Calling web_search function...")
    if is_web_search:
        results = await web_search(query, max_results=5,search_depth="advanced")
    else:
        results = await web_search(query, max_results=2,search_depth="basic")
        
    print(f"[WebSearch] Got {len(results)} results from web_search")
    
    if not results:
        state["response"] = "No web results found or Tavily unavailable."
        print(f"[WebSearch] No results found, returning early")
        return state
    
    await send_status_update(state, f"üìÑ Processing {len(results)} search results...", 40)

    sources_text = "\n".join(
        [f"[Source {i+1}] ({doc.metadata.get('url')})\n"
         f"{doc.page_content[:400]}"
         for i, doc in enumerate(results)]
    )

    user_prompt = f"""User Query: {query}

Search Results:
{sources_text}

Now synthesize them into a clear, structured answer with:
- Headings and subheadings
- Numbered or bulleted lists
- A final 'Sources Used' section with URLs(no titles or anything).
"""
    await send_status_update(state, "ü§ñ Generating response from search results...", 70)
    chunk_callback = state.get("_chunk_callback")
    print(f"[WebSearch] Chunk callback retrieved: {chunk_callback is not None}")
    print(f"[WebSearch] Chunk callback type: {type(chunk_callback)}")
    full_response = ""
    if not is_web_search:
        # llm=ChatGoogleGenerativeAI(
        #         model="gemini-2.5-flash-lite",
        #         temperature=0.3,
        #         google_api_key=google_api_key,
        #     )
        llm=get_llm(llm_model, temperature=0.7)
        system_msg = SystemMessage(content=STATIC_SYS_WEBSEARCH_BASIC)
        human_msg = HumanMessage(content=user_prompt)

        full_response, _ = await stream_with_token_tracking(
            llm,
            [system_msg, human_msg],
            chunk_callback=chunk_callback,
            state=state
        )
        print(f"[WebSearch] Basic search completed")
        
    else:    
        # llm=ChatGoogleGenerativeAI(
        #         model="gemini-2.5-flash-lite",
        #         temperature=0.3,
        #         google_api_key=google_api_key,
        #     )
        llm=get_llm(llm_model, temperature=0.5)
        system_msg = SystemMessage(content=STATIC_SYS_WEBSEARCH)
        human_msg = HumanMessage(content=user_prompt)

        full_response, _ = await stream_with_token_tracking(
            llm,
            [system_msg, human_msg],
            chunk_callback=chunk_callback,
            state=state
        )
        print(f"[WebSearch] Web search completed")
    if chunk_callback:
        await chunk_callback("\n\n")
        full_response += "\n\n"                
    try:
        
        # print(f"[WebSearch] LLM response received: {full_response[:100]}...")
        state["response"] = full_response
        state.setdefault("intermediate_results", []).append({
        "node": "WebSearch",
        "query": query,
        "output": state["response"]
    })
        await send_status_update(state, "‚úÖ Web search completed", 100)

    except Exception as e:
        state["response"] = f"Web search formatting failed: {e}"
        print(f"[WebSearch] LLM error: {e}")
        import traceback
        traceback.print_exc()
    state.setdefault("messages", []).append({
        "role": "assistant",
        "content": state["response"]
    })
    # print(f"[WebSearch] Web search completed, response length: {len(state['response'])}")
    return state
